{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch-02_TF_High_Level_Libraries.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alxfed/time-to-test-new-tensorflow/blob/master/ch_02_TF_High_Level_Libraries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "toc": true,
        "id": "ZViaZj-vpeXl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#TF-Estimator-MNIST-Example\" data-toc-modified-id=\"TF-Estimator-MNIST-Example-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>TF Estimator MNIST Example</a></span></li><li><span><a href=\"#TF-Slim-MNIST-Example\" data-toc-modified-id=\"TF-Slim-MNIST-Example-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>TF Slim MNIST Example</a></span></li><li><span><a href=\"#TFLearn-MNIST-Example\" data-toc-modified-id=\"TFLearn-MNIST-Example-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>TFLearn MNIST Example</a></span></li><li><span><a href=\"#Pretty-Tensor-MNIST-Example\" data-toc-modified-id=\"Pretty-Tensor-MNIST-Example-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Pretty Tensor MNIST Example</a></span></li><li><span><a href=\"#Sonnet-MNIST-Example\" data-toc-modified-id=\"Sonnet-MNIST-Example-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Sonnet MNIST Example</a></span></li></ul></div>"
      ]
    },
    {
      "metadata": {
        "id": "_RSvl75kpeXo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# High-Level Libraries for TensorFlow <a class=\"tocSkip\">"
      ]
    },
    {
      "metadata": {
        "id": "0fOMPc_JpeXq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TF Estimator MNIST Example "
      ]
    },
    {
      "metadata": {
        "id": "Rq2158D1peXs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1633
        },
        "outputId": "5ca9dd89-c78a-4607-d7d4-43c5101ddd83"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "mnist = input_data.read_data_sets(os.path.join('.', 'mnist'),\n",
        "                                  one_hot=False\n",
        "                                  )\n",
        "x_train = mnist.train.images\n",
        "y_train = mnist.train.labels\n",
        "x_test = mnist.test.images\n",
        "y_test = mnist.test.labels\n",
        "\n",
        "n_classes = 10\n",
        "batch_size = 100\n",
        "n_steps = 1000\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "def model_fn(features, labels, mode):\n",
        "    \"\"\" define the model function\n",
        "    \"\"\"\n",
        "    espec_op = tf.estimator.EstimatorSpec\n",
        "    # features is a dict as per Estimator specifications\n",
        "    x = features['images']\n",
        "    # define the network\n",
        "    layer_1 = tf.layers.dense(x, 32)\n",
        "    layer_2 = tf.layers.dense(layer_1, 32)\n",
        "    logits = tf.layers.dense(layer_2, n_classes)\n",
        "\n",
        "    # define predicted classes\n",
        "    predicted_classes = tf.argmax(logits, axis=1)\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        espec = espec_op(mode,\n",
        "                         predictions=predicted_classes\n",
        "                         )\n",
        "    else:\n",
        "        # define loss and optimizer\n",
        "        entropy_op = tf.nn.sparse_softmax_cross_entropy_with_logits\n",
        "        loss_op = tf.reduce_mean(entropy_op(logits=logits,\n",
        "                                            labels=tf.cast(labels,\n",
        "                                                           dtype=tf.int32)\n",
        "                                            )\n",
        "                                 )\n",
        "        optimizer = tf.train.GradientDescentOptimizer(\n",
        "            learning_rate=learning_rate)\n",
        "        train_op = optimizer.minimize(\n",
        "            loss_op, global_step=tf.train.get_global_step())\n",
        "\n",
        "        # define accuracy\n",
        "        accuracy_op = tf.metrics.accuracy(\n",
        "            labels=labels, predictions=predicted_classes)\n",
        "\n",
        "        espec = espec_op(mode=mode,\n",
        "                         predictions=predicted_classes,\n",
        "                         loss=loss_op,\n",
        "                         train_op=train_op,\n",
        "                         eval_metric_ops={'accuracy': accuracy_op}\n",
        "                         )\n",
        "\n",
        "    return espec\n",
        "\n",
        "\n",
        "# create estimator object\n",
        "model = tf.estimator.Estimator(model_fn)\n",
        "\n",
        "# train the model\n",
        "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x={'images': x_train},\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=None,\n",
        "    shuffle=True)\n",
        "model.train(train_input_fn, steps=n_steps)\n",
        "\n",
        "# evaluate the model\n",
        "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x={'images': x_test},\n",
        "    y=y_test,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False)\n",
        "model.evaluate(eval_input_fn)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-4d4e8a4ad79c>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp6eh_xrng\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp6eh_xrng', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbee03ea2b0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From <ipython-input-1-4d4e8a4ad79c>:29: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py:455: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py:809: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp6eh_xrng/model.ckpt.\n",
            "INFO:tensorflow:loss = 2.4529204, step = 0\n",
            "INFO:tensorflow:global_step/sec: 358.654\n",
            "INFO:tensorflow:loss = 1.3011882, step = 100 (0.284 sec)\n",
            "INFO:tensorflow:global_step/sec: 382.261\n",
            "INFO:tensorflow:loss = 0.7431646, step = 200 (0.258 sec)\n",
            "INFO:tensorflow:global_step/sec: 358.395\n",
            "INFO:tensorflow:loss = 0.6314546, step = 300 (0.282 sec)\n",
            "INFO:tensorflow:global_step/sec: 404.599\n",
            "INFO:tensorflow:loss = 0.83477485, step = 400 (0.244 sec)\n",
            "INFO:tensorflow:global_step/sec: 393.397\n",
            "INFO:tensorflow:loss = 0.6209571, step = 500 (0.258 sec)\n",
            "INFO:tensorflow:global_step/sec: 414.007\n",
            "INFO:tensorflow:loss = 0.45819455, step = 600 (0.238 sec)\n",
            "INFO:tensorflow:global_step/sec: 379.03\n",
            "INFO:tensorflow:loss = 0.48781052, step = 700 (0.266 sec)\n",
            "INFO:tensorflow:global_step/sec: 388.867\n",
            "INFO:tensorflow:loss = 0.42082423, step = 800 (0.257 sec)\n",
            "INFO:tensorflow:global_step/sec: 402.016\n",
            "INFO:tensorflow:loss = 0.2916147, step = 900 (0.247 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmp6eh_xrng/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.25037962.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-02-07T15:50:16Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmp6eh_xrng/model.ckpt-1000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2019-02-07-15:50:16\n",
            "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.8901, global_step = 1000, loss = 0.39166576\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /tmp/tmp6eh_xrng/model.ckpt-1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.8901, 'global_step': 1000, 'loss': 0.39166576}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "T8dY56I4peX4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TF Slim MNIST Example"
      ]
    },
    {
      "metadata": {
        "id": "zli53up5peX6",
        "colab_type": "code",
        "colab": {},
        "outputId": "06dc60c9-8b72-464e-ec7a-586fdbb31e83"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from tensorflow.contrib import slim\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "n_classes = 10  \n",
        "n_steps = 1000\n",
        "\n",
        "# let us get the data\n",
        "mnist = input_data.read_data_sets(os.path.join('.', 'mnist'), one_hot=True)\n",
        "\n",
        "X_train = mnist.train.images\n",
        "X_train = tf.convert_to_tensor(X_train)\n",
        "Y_train = mnist.train.labels\n",
        "Y_train = tf.convert_to_tensor(Y_train)\n",
        "\n",
        "\n",
        "def mlp(x):\n",
        "    net = slim.fully_connected(x, 32, scope='fc1')\n",
        "    net = slim.dropout(net, 0.5, scope='dropout1')\n",
        "    net = slim.fully_connected(net, 32, scope='fc2')\n",
        "    net = slim.dropout(net, 0.5, scope='dropout2')\n",
        "    net = slim.fully_connected(net, n_classes, activation_fn=None, scope='fc3')\n",
        "    return net\n",
        "\n",
        "\n",
        "# Define the model\n",
        "logits = mlp(X_train)\n",
        "\n",
        "# Define the loss functions and get the total loss\n",
        "loss = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=Y_train)\n",
        "total_loss = tf.losses.get_total_loss()\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "\n",
        "train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
        "\n",
        "# Run the training\n",
        "final_loss = slim.learning.train(\n",
        "    train_op,\n",
        "    logdir='./slim_logs',\n",
        "    number_of_steps=n_steps,\n",
        "    log_every_n_steps=100)\n",
        "\n",
        "print('final loss={}'.format(final_loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
            "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/training/python/training/training.py:412: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "INFO:tensorflow:Starting Session.\n",
            "INFO:tensorflow:Saving checkpoint to path ./slim_logs/model.ckpt\n",
            "INFO:tensorflow:global_step/sec: 0\n",
            "INFO:tensorflow:Starting Queues.\n",
            "INFO:tensorflow:global step 100: loss = 2.2669 (0.010 sec/step)\n",
            "INFO:tensorflow:global step 200: loss = 2.2025 (0.010 sec/step)\n",
            "INFO:tensorflow:global step 300: loss = 2.1257 (0.010 sec/step)\n",
            "INFO:tensorflow:global step 400: loss = 2.0419 (0.009 sec/step)\n",
            "INFO:tensorflow:global step 500: loss = 1.9532 (0.009 sec/step)\n",
            "INFO:tensorflow:global step 600: loss = 1.8733 (0.010 sec/step)\n",
            "INFO:tensorflow:global step 700: loss = 1.8002 (0.010 sec/step)\n",
            "INFO:tensorflow:global step 800: loss = 1.7273 (0.010 sec/step)\n",
            "INFO:tensorflow:global step 900: loss = 1.6688 (0.010 sec/step)\n",
            "INFO:tensorflow:global step 1000: loss = 1.6132 (0.010 sec/step)\n",
            "INFO:tensorflow:Stopping Training.\n",
            "INFO:tensorflow:Finished training! Saving model to disk.\n",
            "final loss=1.6131552457809448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mW8ZARQYpeYD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TFLearn MNIST Example"
      ]
    },
    {
      "metadata": {
        "id": "dW7YxHD_peYF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "\n",
        "import tflearn\n",
        "import tflearn.datasets.mnist as mnist\n",
        "import os\n",
        "\n",
        "batch_size = 100\n",
        "n_classes = 10\n",
        "n_epochs = 10\n",
        "\n",
        "X_train, Y_train, X_test, Y_test = mnist.load_data(\n",
        "    data_dir=os.path.join('.', 'mnist'), one_hot=True)\n",
        "\n",
        "# Build deep neural network\n",
        "input_layer = tflearn.input_data(shape=[None, 784])\n",
        "layer1 = tflearn.fully_connected(input_layer,\n",
        "                                 10,\n",
        "                                 activation='relu'\n",
        "                                 )\n",
        "layer2 = tflearn.fully_connected(layer1,\n",
        "                                 10,\n",
        "                                 activation='relu'\n",
        "                                 )\n",
        "output = tflearn.fully_connected(layer2,\n",
        "                                 n_classes,\n",
        "                                 activation='softmax'\n",
        "                                 )\n",
        "\n",
        "net = tflearn.regression(output,\n",
        "                         optimizer='adam',\n",
        "                         metric=tflearn.metrics.Accuracy(),\n",
        "                         loss='categorical_crossentropy'\n",
        "                         )\n",
        "model = tflearn.DNN(net)\n",
        "\n",
        "model.fit(\n",
        "    X_train,\n",
        "    Y_train,\n",
        "    n_epoch=n_epochs,\n",
        "    batch_size=batch_size,\n",
        "    show_metric=True,\n",
        "    run_id='dense_model')\n",
        "\n",
        "score = model.evaluate(X_test, Y_test)\n",
        "print('Test accuracy:', score[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fvFOd3jhpeYK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pretty Tensor MNIST Example"
      ]
    },
    {
      "metadata": {
        "id": "WVuRjROfpeYM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "\n",
        "import numpy as np\n",
        "from __future__ import division\n",
        "\n",
        "import prettytensor as pt\n",
        "from prettytensor.tutorial import data_utils\n",
        "import os\n",
        "data_utils.WORK_DIRECTORY = os.path.join('.', 'mnist')\n",
        "\n",
        "# get the data\n",
        "X_train, Y_train = data_utils.mnist(training=True)\n",
        "X_test, Y_test = data_utils.mnist(training=False)\n",
        "\n",
        "# define hyperparameters\n",
        "\n",
        "batch_size = 100     # number of samples that would\n",
        "# be used to learn the parameters in a batch\n",
        "n_classes = 10       # number of outputs, i.e. digits 0 to 9\n",
        "n_epochs = 10        # number of ietrations for learning the parameters\n",
        "n_batches = int(X_train.shape[0] / batch_size)\n",
        "n_samples_in_train_batch = 60000 // batch_size\n",
        "n_samples_in_test_batch = 10000 // batch_size\n",
        "\n",
        "# define inputs and outputs\n",
        "\n",
        "X = tf.placeholder(tf.float32, [batch_size, 28, 28, 1])\n",
        "Y = tf.placeholder(tf.float32, [batch_size, 10])\n",
        "\n",
        "# define the model\n",
        "\n",
        "X = pt.wrap(X)\n",
        "\n",
        "model = (X.\n",
        "         flatten().\n",
        "         fully_connected(10).\n",
        "         softmax_classifier(n_classes, labels=Y)\n",
        "         )\n",
        "\n",
        "# define evaluator (metrics), optimizer and training functions\n",
        "\n",
        "evaluator = model.softmax.evaluate_classifier(Y)\n",
        "optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
        "trainer = pt.apply_optimizer(optimizer, losses=[model.loss])\n",
        "\n",
        "runner = pt.train.Runner()\n",
        "\n",
        "with tf.Session() as tfs:\n",
        "    for epoch in range(0, n_epochs):\n",
        "        # shuffle the training data\n",
        "        X_train, Y_train = data_utils.permute_data((X_train, Y_train))\n",
        "\n",
        "        runner.train_model(\n",
        "            trainer,\n",
        "            model.loss,\n",
        "            n_samples_in_train_batch,\n",
        "            feed_vars=(X, Y),\n",
        "            feed_data=pt.train.feed_numpy(batch_size, X_train, Y_train),\n",
        "            print_every=600\n",
        "        )\n",
        "\n",
        "        score = runner.evaluate_model(\n",
        "            evaluator,\n",
        "            n_samples_in_test_batch,\n",
        "            feed_vars=(X, Y),\n",
        "            feed_data=pt.train.feed_numpy(batch_size, X_test, Y_test)\n",
        "        )\n",
        "\n",
        "        print('Accuracy after {} epochs {} \\n'.\n",
        "              format(epoch + 1, score[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9HBKmBvQpeYR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sonnet MNIST Example"
      ]
    },
    {
      "metadata": {
        "id": "OFNITMgOpeYS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "\n",
        "import os\n",
        "import sonnet as snt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n",
        "class MNIST(snt.AbstractModule):\n",
        "\n",
        "    def __init__(self, mnist_part, batch_size, name='MNIST'):\n",
        "\n",
        "        super(MNIST, self).__init__(name=name)\n",
        "\n",
        "        self._X = tf.constant(mnist_part.images, dtype=tf.float32)\n",
        "        self._Y = tf.constant(mnist_part.labels, dtype=tf.float32)\n",
        "        self._batch_size = batch_size\n",
        "        self._M = mnist_part.num_examples\n",
        "\n",
        "    def _build(self):\n",
        "        idx = tf.random_uniform([self._batch_size], 0, self._M, tf.int64)\n",
        "        X = tf.gather(self._X, idx)\n",
        "        Y = tf.gather(self._Y, idx)\n",
        "        return X, Y\n",
        "\n",
        "\n",
        "class MLP(snt.AbstractModule):\n",
        "    def __init__(self, output_sizes, name='mlp'):\n",
        "        super(MLP, self).__init__(name=name)\n",
        "\n",
        "        self._layers = []\n",
        "\n",
        "        for output_size in output_sizes:\n",
        "            self._layers.append(snt.Linear(output_size=output_size))\n",
        "\n",
        "    def _build(self, X):\n",
        "\n",
        "        # add the input layer\n",
        "        model = tf.sigmoid(self._layers[0](X))\n",
        "\n",
        "        # add hidden layers\n",
        "        for i in range(1, len(self._layers) - 1):\n",
        "            model = tf.sigmoid(self._layers[i](model))\n",
        "\n",
        "        # add output layer\n",
        "        model = tf.nn.softmax(self._layers[len(self._layers) - 1](model))\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "batch_size = 100\n",
        "n_classes = 10\n",
        "n_epochs = 10\n",
        "\n",
        "mnist = input_data.read_data_sets(os.path.join('.', 'mnist'),\n",
        "                                  one_hot=True\n",
        "                                  )\n",
        "train = MNIST(mnist.train, batch_size=batch_size)\n",
        "test = MNIST(mnist.test, batch_size=batch_size)\n",
        "\n",
        "X_train, Y_train = train()\n",
        "X_test, Y_test = test()\n",
        "\n",
        "model = MLP([20, n_classes])\n",
        "\n",
        "Y_train_hat = model(X_train)\n",
        "Y_test_hat = model(X_test)\n",
        "\n",
        "\n",
        "def loss(Y_hat, Y):\n",
        "    return -tf.reduce_sum(Y * tf.log(Y_hat))\n",
        "\n",
        "\n",
        "L_train = loss(Y_train_hat, Y_train)\n",
        "L_test = loss(Y_test_hat, Y_test)\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(\n",
        "    learning_rate=0.01).minimize(L_train)\n",
        "\n",
        "with tf.Session() as tfs:\n",
        "    tf.global_variables_initializer().run()\n",
        "    for epoch in range(n_epochs):\n",
        "        loss_val, _ = tfs.run((L_train, optimizer))\n",
        "        print('Epoch : {} Training Loss : {}'.format(epoch, loss_val))\n",
        "\n",
        "    loss_val = tfs.run(L_test)\n",
        "    print('Test loss : {}'.format(loss_val))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}